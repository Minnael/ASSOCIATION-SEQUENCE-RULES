# Algoritmos de MineraÃ§Ã£o de Dados

Este repositÃ³rio contÃ©m implementaÃ§Ãµes de algoritmos clÃ¡ssicos de mineraÃ§Ã£o de dados para descoberta de padrÃµes frequentes e regras de associaÃ§Ã£o.

---

## ğŸ“š Ãndice

1. [Apriori](#apriori)
2. [FP-Growth](#fp-growth)
3. [GSP - Generalized Sequential Pattern](#gsp)
4. [PrefixSpan](#prefixspan)

---

## ğŸ” Apriori

### DescriÃ§Ã£o
O **Apriori** Ã© um algoritmo clÃ¡ssico para mineraÃ§Ã£o de regras de associaÃ§Ã£o em bases de dados transacionais. Ele identifica conjuntos de itens frequentes e gera regras de associaÃ§Ã£o baseadas em suporte e confianÃ§a mÃ­nimos.

### FÃ³rmulas MatemÃ¡ticas

#### Suporte
Mede a frequÃªncia de um itemset no dataset:

$$\text{Suporte}(X) = \frac{\text{NÃºmero de transaÃ§Ãµes contendo } X}{\text{Total de transaÃ§Ãµes}}$$

#### ConfianÃ§a
Mede a forÃ§a da regra $X \rightarrow Y$:

$$\text{ConfianÃ§a}(X \rightarrow Y) = \frac{\text{Suporte}(X \cup Y)}{\text{Suporte}(X)}$$

#### Lift
Mede a independÃªncia entre X e Y:

$$\text{Lift}(X \rightarrow Y) = \frac{\text{ConfianÃ§a}(X \rightarrow Y)}{\text{Suporte}(Y)}$$

- **Lift > 1**: X e Y sÃ£o positivamente correlacionados
- **Lift = 1**: X e Y sÃ£o independentes
- **Lift < 1**: X e Y sÃ£o negativamente correlacionados

### Funcionamento

1. **L1**: Encontra itemsets de tamanho 1 que atendem ao suporte mÃ­nimo
2. **GeraÃ§Ã£o de Candidatos**: Combina itemsets de tamanho k para gerar candidatos de tamanho k+1
3. **Poda**: Remove candidatos que nÃ£o atendem ao suporte mÃ­nimo
4. **IteraÃ§Ã£o**: Repete atÃ© nÃ£o haver mais itemsets frequentes
5. **Regras**: Gera regras de associaÃ§Ã£o a partir dos itemsets frequentes

### Resultados (Dataset de Exemplo)

**Dataset**: 8 transaÃ§Ãµes com itens {tomate, cerveja, arroz, frango, mamadeira, pera}

**ParÃ¢metros**:
- Suporte mÃ­nimo: 0.375 (37.5%)
- ConfianÃ§a mÃ­nima: 0.60 (60%)

**Itemsets Frequentes**:
- **L1**: {cerveja}, {arroz}, {tomate}, {mamadeira}
- **L2**: {cerveja, arroz}, {tomate, cerveja}, {tomate, arroz}
- **L3**: {tomate, cerveja, arroz}

**Exemplos de Regras**:
- {cerveja} â†’ {arroz} | confianÃ§a=0.86 | suporte=0.50
- {tomate} â†’ {cerveja, arroz} | confianÃ§a=0.75 | suporte=0.375

---

## ğŸŒ³ FP-Growth

### DescriÃ§Ã£o
O **FP-Growth** (Frequent Pattern Growth) Ã© um algoritmo mais eficiente que o Apriori, pois evita a geraÃ§Ã£o de candidatos. Utiliza uma estrutura de dados compacta chamada **FP-Tree** para armazenar informaÃ§Ãµes sobre transaÃ§Ãµes frequentes.

### FÃ³rmulas MatemÃ¡ticas

Utiliza as mesmas mÃ©tricas do Apriori:
- **Suporte**: $\text{Suporte}(X) = \frac{\text{count}(X)}{N}$
- **ConfianÃ§a**: $\text{ConfianÃ§a}(X \rightarrow Y) = \frac{\text{Suporte}(X \cup Y)}{\text{Suporte}(X)}$
- **Lift**: $\text{Lift}(X \rightarrow Y) = \frac{\text{ConfianÃ§a}(X \rightarrow Y)}{\text{Suporte}(Y)}$

### Funcionamento

1. **Primeira Varredura**: Conta a frequÃªncia de cada item
2. **OrdenaÃ§Ã£o**: Ordena itens por frequÃªncia decrescente
3. **ConstruÃ§Ã£o da FP-Tree**: 
   - Cria uma Ã¡rvore compacta com nÃ³s compartilhados
   - MantÃ©m uma tabela de cabeÃ§alhos (header table) para acesso rÃ¡pido
4. **MineraÃ§Ã£o Recursiva**:
   - Para cada item, constrÃ³i uma base de padrÃµes condicionais
   - Gera FP-Tree condicional
   - Extrai padrÃµes frequentes recursivamente

### Vantagens sobre Apriori
- Apenas 2 varreduras no dataset
- NÃ£o gera candidatos explicitamente
- Estrutura compacta em memÃ³ria
- Mais eficiente para datasets grandes

### Resultados (Dataset de Exemplo)

**Dataset**: 10 transaÃ§Ãµes com itens {a, b, c, d, e}

**ParÃ¢metros**:
- Suporte mÃ­nimo: 2 transaÃ§Ãµes
- ConfianÃ§a mÃ­nima: 0.60

**Itemsets Frequentes Encontrados**:
- Tamanho 1: {a}â†’9, {b}â†’7, {c}â†’6, {d}â†’6, {e}â†’3
- Tamanho 2: {a,b}â†’6, {a,c}â†’5, {a,d}â†’5, {b,c}â†’5, {b,d}â†’4, {c,d}â†’3
- Tamanho 3: {a,b,c}â†’4, {a,b,d}â†’3, {a,c,d}â†’2, {b,c,d}â†’2
- Tamanho 4: {a,b,c,d}â†’2

**Exemplos de Regras**:
- {b} â†’ {a} | suporte=6, confianÃ§a=0.86, lift=0.95
- {c,d} â†’ {a} | suporte=2, confianÃ§a=0.67, lift=0.74

---

## ğŸ“Š GSP (Generalized Sequential Pattern)

### DescriÃ§Ã£o
O **GSP** Ã© um algoritmo para mineraÃ§Ã£o de padrÃµes sequenciais. Diferente do Apriori, que trabalha com conjuntos (sem ordem), o GSP considera a **ordem temporal** dos eventos, identificando sequÃªncias frequentes.

### Conceitos

- **SequÃªncia**: Lista ordenada de eventos
- **Evento (itemset)**: Conjunto de itens que ocorrem simultaneamente
- **SubsequÃªncia**: Uma sequÃªncia S Ã© subsequÃªncia de T se todos os elementos de S aparecem em T na mesma ordem

**Exemplo**: 
- SequÃªncia: `<{pÃ£o}, {leite}, {manteiga}>`
- `<{pÃ£o}, {manteiga}>` Ã© subsequÃªncia vÃ¡lida
- `<{manteiga}, {pÃ£o}>` NÃƒO Ã© subsequÃªncia (ordem invertida)

### FÃ³rmulas MatemÃ¡ticas

#### Suporte de SequÃªncia
$$\text{Suporte}(S) = \frac{\text{NÃºmero de clientes que contÃªm a sequÃªncia } S}{\text{Total de clientes}}$$

#### ConfianÃ§a de Regra Sequencial
Para uma regra $S_1 \Rightarrow S_2$:

$$\text{ConfianÃ§a}(S_1 \Rightarrow S_2) = \frac{\text{Suporte}(S_1 \cdot S_2)}{\text{Suporte}(S_1)}$$

onde $S_1 \cdot S_2$ representa a concatenaÃ§Ã£o das sequÃªncias.

### Funcionamento

1. **Fase 1**: Encontra sequÃªncias de tamanho 1 frequentes
2. **GeraÃ§Ã£o de Candidatos**: Combina sequÃªncias de tamanho k para gerar k+1
   - **Join**: Junta duas sequÃªncias se compartilham k-1 elementos
3. **Poda**: Remove candidatos que nÃ£o atingem suporte mÃ­nimo
4. **IteraÃ§Ã£o**: Repete atÃ© nÃ£o haver mais padrÃµes frequentes
5. **Regras**: Gera regras sequenciais com base na confianÃ§a

### Resultados (Dataset de Exemplo)

**Dataset**: 5 clientes com sequÃªncias de compras

```
Cliente 1: <{pÃ£o}, {leite}, {manteiga}>
Cliente 2: <{pÃ£o}, {leite}>
Cliente 3: <{pÃ£o}, {cerveja}>
Cliente 4: <{leite}, {manteiga}>
Cliente 5: <{pÃ£o}, {leite}, {manteiga}>
```

**ParÃ¢metros**:
- Suporte mÃ­nimo: 2 (40% dos clientes)
- ConfianÃ§a mÃ­nima: 0.60

**PadrÃµes Sequenciais Frequentes**:
- Tamanho 1: `<{pÃ£o}>`, `<{leite}>`, `<{manteiga}>`
- Tamanho 2: `<{pÃ£o}, {leite}>`, `<{leite}, {manteiga}>`
- Tamanho 3: `<{pÃ£o}, {leite}, {manteiga}>`

**Regras Sequenciais**:
- `<{pÃ£o}>` â‡’ `<{leite}>` | confianÃ§a=0.75
- `<{leite}>` â‡’ `<{manteiga}>` | confianÃ§a=0.67
- `<{pÃ£o}, {leite}>` â‡’ `<{manteiga}>` | confianÃ§a=0.67

---

## ğŸ”— PrefixSpan

### DescriÃ§Ã£o
O **PrefixSpan** (Prefix-Projected Sequential Pattern Mining) Ã© um algoritmo mais eficiente que o GSP para mineraÃ§Ã£o de padrÃµes sequenciais. Ele usa uma abordagem de **crescimento de padrÃ£o** e **projeÃ§Ã£o de banco de dados**, evitando a geraÃ§Ã£o de candidatos.

### FÃ³rmulas MatemÃ¡ticas

Utiliza as mesmas mÃ©tricas do GSP:

#### Suporte
$$\text{Suporte}(S) = \frac{|\{sid \in D \mid S \subseteq sid\}|}{|D|}$$

onde $D$ Ã© o conjunto de todas as sequÃªncias e $S \subseteq sid$ indica que S Ã© subsequÃªncia de sid.

#### ConfianÃ§a
$$\text{ConfianÃ§a}(\alpha \Rightarrow \beta) = \frac{\text{Suporte}(\alpha \cdot \beta)}{\text{Suporte}(\alpha)}$$

### Funcionamento

1. **Busca em Profundidade**: Explora o espaÃ§o de busca em profundidade
2. **DivisÃ£o e Conquista**: 
   - Divide o problema em subproblemas menores
   - Para cada prefixo frequente, cria um banco de dados projetado
3. **ProjeÃ§Ã£o**: 
   - Projeta o banco de dados com base no prefixo atual
   - Reduz o tamanho do problema progressivamente
4. **RecursÃ£o**: Minera padrÃµes nos bancos projetados

### Vantagens sobre GSP
- **NÃ£o gera candidatos**: Evita explosÃ£o combinatÃ³ria
- **Busca em profundidade**: Mais eficiente em memÃ³ria
- **ProjeÃ§Ã£o de DB**: Reduz progressivamente o tamanho do problema
- **Mais rÃ¡pido**: Especialmente para sequÃªncias longas

### Resultados (Dataset de Exemplo)

**Dataset**: 5 clientes com sequÃªncias

```
Cliente 1: ["pao", "leite", "manteiga"]
Cliente 2: ["pao", "leite"]
Cliente 3: ["pao", "cerveja"]
Cliente 4: ["leite", "manteiga"]
Cliente 5: ["pao", "leite", "manteiga"]
```

**ParÃ¢metros**:
- Suporte mÃ­nimo: 2
- ConfianÃ§a mÃ­nima: 0.60
- minlen: 1, maxlen: 10

**PadrÃµes Frequentes**:
- ('pao',) | suporte=4
- ('leite',) | suporte=4
- ('manteiga',) | suporte=3
- ('pao', 'leite') | suporte=3
- ('leite', 'manteiga') | suporte=3
- ('pao', 'leite', 'manteiga') | suporte=2

**Regras Sequenciais**:
- ('pao',) â†’ ('leite',) | suporte=3 | confianÃ§a=0.75
- ('leite',) â†’ ('manteiga',) | suporte=3 | confianÃ§a=0.75
- ('pao', 'leite') â†’ ('manteiga',) | suporte=2 | confianÃ§a=0.67

---

## ğŸ“Š ComparaÃ§Ã£o dos Algoritmos

| CaracterÃ­stica | Apriori | FP-Growth | GSP | PrefixSpan |
|---------------|---------|-----------|-----|------------|
| **Tipo** | Itemsets | Itemsets | Sequencial | Sequencial |
| **Gera Candidatos** | Sim | NÃ£o | Sim | NÃ£o |
| **Estrutura** | Lista | FP-Tree | Lista | ProjeÃ§Ã£o |
| **Varreduras DB** | MÃºltiplas | 2 | MÃºltiplas | 1 + ProjeÃ§Ãµes |
| **Ordem Importa** | NÃ£o | NÃ£o | Sim | Sim |
| **EficiÃªncia** | Baixa | Alta | MÃ©dia | Alta |
| **MemÃ³ria** | Baixa | MÃ©dia | Baixa | Baixa |

### Quando Usar Cada Algoritmo?

- **Apriori**: Datasets pequenos, didÃ¡tico, quando se quer entender o funcionamento bÃ¡sico
- **FP-Growth**: Datasets grandes, quando nÃ£o hÃ¡ ordem temporal, melhor performance
- **GSP**: Quando a ordem dos eventos Ã© importante, anÃ¡lise de comportamento sequencial
- **PrefixSpan**: Grandes volumes de dados sequenciais, melhor performance que GSP

---

## ğŸ› ï¸ Estrutura do RepositÃ³rio

```
CODES/
â”œâ”€â”€ A-PRIORI/
â”‚   â”œâ”€â”€ apriori.py        # ImplementaÃ§Ã£o manual
â”‚   â”œâ”€â”€ library.py        # Usando MLxtend
â”‚   â””â”€â”€ main.py           # ExecuÃ§Ã£o principal
â”œâ”€â”€ FP-GROWTH/
â”‚   â”œâ”€â”€ fp-growth.py      # ImplementaÃ§Ã£o manual
â”‚   â””â”€â”€ library.py        # Usando MLxtend
â”œâ”€â”€ GSP/
â”‚   â””â”€â”€ GSP.py            # ImplementaÃ§Ã£o completa
â”œâ”€â”€ PREFIXSPAN/
â”‚   â””â”€â”€ library.py        # Usando biblioteca prefixspan
â””â”€â”€ README.md             # Este arquivo
```

---

## ğŸ“¦ DependÃªncias

```bash
pip install pandas
pip install mlxtend
pip install prefixspan
```

---

## ğŸš€ Como Executar

### Apriori
```bash
cd A-PRIORI
python main.py
# ou
python library.py
```

### FP-Growth
```bash
cd FP-GROWTH
python fp-growth.py
# ou
python library.py
```

### GSP
```bash
cd GSP
python GSP.py
```

### PrefixSpan
```bash
cd PREFIXSPAN
python library.py
```

---

## ğŸ“– ReferÃªncias

- Agrawal, R., & Srikant, R. (1994). Fast algorithms for mining association rules. *VLDB*.
- Han, J., Pei, J., & Yin, Y. (2000). Mining frequent patterns without candidate generation. *SIGMOD*.
- Srikant, R., & Agrawal, R. (1996). Mining sequential patterns: Generalizations and performance improvements. *EDBT*.
- Pei, J., et al. (2001). PrefixSpan: Mining sequential patterns efficiently by prefix-projected pattern growth. *ICDE*.

---

## ğŸ‘¨â€ğŸ“ Contexto AcadÃªmico

Este repositÃ³rio foi desenvolvido como parte de estudos de Mestrado em CiÃªncia da ComputaÃ§Ã£o, focando em tÃ©cnicas de MineraÃ§Ã£o de Dados e Descoberta de Conhecimento em Bases de Dados (KDD).

---

## ğŸ“ LicenÃ§a

Material acadÃªmico para fins educacionais.
